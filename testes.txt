Cross-Validation (base com 229 imagens) k=10 s/ repetições

TensorFlow v1: 
65,93%

TensorFlow v2: 
41,98% (queda inexplicável)

Otimizações:
c/ camadas relu com kernel_initializerinitializer = he_uniform, padding = same, e batch = 5:  
66,34%

Colocar mais dados aumentados (fator de 32)
c/ modificações anteriores, batch = 32 e steps_per_epoch = total_train:
71.56% (+/- 9.98%)
73.75% (+/- 9.68%)

Com modificações anteriores e otimizador 'adam':
81.23% (+/- 9.72%)

Com modificações anteriores, mas sem callback:
82.53% (+/- 5.84%)

Voltando com o callback e c/ GPU (Precision):
75.91% (+/- 10.43%)

Aumentando a base para 268 imagens (Precision GPU):
78.39% (+/- 8.18%)

VGG16 falhou com GPU no Snoopy.
VGG16 - otimizador SGD (Snoopy)
muito lento com configurações anteriores, então
batch = 8, steps_per_epoch = total_train / batch_size, e sem aumento de dados

GPU do Precision é pelo menos 50 vezes mais rápida que CPU do Snoopy
VGG16 - otimizador SGD (Precision GPU): 
82.11% (+/- 7.82%)

VGG16 - otimizador adam (Precision GPU):
82.81% (+/- 5.18%)

VGG19 - otimizador SGD (Precision GPU):
83.96% (+/- 5.75%)  (erro: pesos do VGG16)

VGG19 - otimizador adam (Precision GPU):
82.09% (+/- 11.91%) (erro: pesos do VGG16)

VGG19 c/ batch=32, steps_per_epoch = total_train, c/ aumento de dados (Precision GPU):
84.30% (+/- 9.72%)  (erro: pesos do VGG16)

Protocolo experimental:
kfold com 10 folds e 10 repetições = 100
batch = 4, sem aumento de dados, otimizador SGD
(batch=4 para caber na memória da GPU do Snoopy - GTX 970 c/ 4 GB)

Aplicando protocolo experimental na rede baseline (s/ aumento de dados): (Donald)
60.45% (incompleto)

Aplicando protocolo experimental na rede baseline (c/ aumento de dados): (Patolino)
67.82% (incompleto)

Aplicando protocolo experimental na rede baseline (c/ aumento de dados * 4 - steps_per_epoch = total_train): (Natcomp02)
74.96% (incompleto)

Aumento da base para 342 itens

baseline c/ adam:    67.97% (+/- 7.33%)
baseline c/ rmsprop: 70.35% (+/- 8.67%)
baseline c/ sgd:     60.53% (+/- 8.64%)
 
baseline c/ adam e data augmentation:    74.35% (+/- 6.90%)
baseline c/ rmsprop e data augmentation: 73.68% (+/- 6.25%)
baseline c/ sgd e data agumentation:     66.56% (+/- 7.39%)

baseline c/ adam e data augmentation *4:    73.51% (+/- 7.98%)
baseline c/ rmsprop e data augmentation *4: 76.40% (+/- 7.14%)
baseline c/ sgd e data augmentation *4:     72.19% (+/- 7.57%)

(transfer learning c/ uma única rodada de cross-validation)
transfer learning c/ rmsprop: Só VGG16 e VGG19 acima de 80% (NASNetLarge não roda na GPU do PRECISION)
transfer learning c/ rmsprop e camada densa alternativa: PATOLINO (Interrompido)
transfer learning c/ rmsprop e pooling avg: NATCOMP02 (Interrompido)
transfer learning c/ rmsprop e pooling max: SNOOPY (Interrompido)

transfer learning c/ rmsprop e sem pesos travados, pooling 'avg': melhor Xception c/ 92.11% (+/- 5.01%)
transfer learning c/ rmsprop e sem pesos travados, s/ poolin: PRECISION

transfer learning c/ 10 rodadas de cross-validation (apenas com melhores modelos: VGG16, VGG19):
padrão: 					VGG16: 86.36% (+/- 7.09%)  VGG19: 87.47% (+/- 6.82%)
camada densa alternativa:	VGG16: 86.56% (+/- 6.07%)  VGG19: 86.91% (+/- 6.59%)
pooling avg:				VGG16: 74.35% (+/- 6.68%)  VGG19: 73.85% (+/- 6.79%)
pooling max:				VGG16: 70.50% (+/- 7.80%)  VGG19: 74.81% (+/- 6.61%)

transfer learning e data augmentation *4: 
VGG16: 86.89% (+/- 6.62%)  
VGG19: 87.32% (+/- 6.43%)

transfer learning e fine-tuning (1 block):
VGG16: 87.10% (+/- 5.84%)
VGG19: 87.00% (+/- 7.13%)

transfer learning e fine-tuning (2 blocks): (MELHOR!)
VGG16: 89.40% (+/- 6.50%)
VGG19: 88.86% (+/- 6.69%)

transfer learning e fine-tuning (3 blocks):
VGG16: 88.96% (+/- 5.99%)
VGG19: 88.34% (+/- 5.91%)

transfer learning c/ data augmentation *4 e fine-tuning (2 blocks):
VGG16: 87.17% (+/- 6.46%)
VGG19: 88.02% (+/- 6.57%)

transfer learning e fine-tuning (2 blocks) c/ 'adam':
VGG16: 86.92% (+/- 5.66%)
VGG19: 88.01% (+/- 5.14%)

transfer learning e fine-tuning (2 blocks) c/ 'sgd':
VGG16: 73.85% (+/- 6.53%)
VGG19: 

transfer learning e fine-tuning (2 blocks) c/ camada densa alternativa:
VGG16: 85.85% (+/- 6.46%)
VGG19: 87.27% (+/- 6.60%)

melhor configuração s/ early stop (ou sem callbacks?):

(eventuais testes com outros otimizadores)

VGG16 - 25088 features; removendo features com std(0): 22878 features
VGG19 - 25088 features; removendo features com std(0): 22216 features

Todos PCC com 100 repetições e 20% rotulado:

PCC c/ PCA-10; e k variável (1 a 100): 
VGG16 k=7: 79.32% (+/- 2.39%)  VGG19 k=7: 79.54% (+/- 2.40%)

PCC c/ k=7, e PCA variável (1 a 100): 
VGG16 PCA=10: 79.16% (+/- 2.84%)  VGG19 PCA=10: 79.19% (+/- 2.84%)

PCC c/ k = 1 a 20 PCA Comps. = 1 a 20: (fazer um heatmap)
VGG16: 79.53% (+/- 2.40%)  (PCA=10; k=7)
VGG19: 79.35% (+/- 2.65%)  (PCA=10; k=8)

Repetindo com 10% rotulado:
VGG16: 77.01% (+/- 3.55%)  (PCA=10; k=7)
VGG19: 76.99% (+/- 3.60%)  (PCA=10; k=8)

Repetindo com 50% rotulados:
VGG16: 82.20% (+/- 2.74%)  (PCA=18; k=2)
VGG19: 81.83% (+/- 2.86%)  (PCA=14; k=3)

Repetindo com 90% rotulados:
VGG16: 84.59% (+/- 5.53%)  (PCA=16; k=2)
VGG19: 84.62% (+/- 5.37%)  (PCA=16; k=2)

PCC em bases com Global Max Pooling:
VGG 16 - 512 features; removendo features com std(0): 508 features
VGG 19 - 512 features; removendo features com std(0): 507 features

PCC c/ k = 1 a 20 PCA Comps. = 1 a 20: (20% rotulado)
VGG16: 74.30% (+/- 2.80%)  (PCA=7; k=9)
VGG19: 72.28% (+/- 3.87%)  (PCA=20; k=6)

PCC em bases com Global Average Pooling:
VGG 16 - 512 features; removendo features com std(0): 508 features
VGG 19 - 512 features; removendo features com std(0): 507 features

VGG16: 72.51% (+/- 3.04%)  (PCA=7; k=6)
VGG19: 71.52% (+/- 3.28%)  (PCA=15; k=3)

PCC em VGG16 + VGG19 sem pooling:
10% rotulado: 76.99% (+/- 3.68%)  (PCA=10; k=8)
20% rotulado: 79.43% (+/- 2.65%)  (PCA=14; k=4)

PCC em VGG16 + VGG19 c/ pooling (10% rotulado):
Global Average Pooling:  69.44% (+/- 4.78%)  (PCA=10; k=6)
Global Max Pooling: 	69.62% (+/- 4.66%)  (PCA=15; k=4)

PCC em VGG16 + VGG19 c/ pooling (20% rotulado):
Global Average Pooling: 73.43% (+/- 3.10%)  (PCA=10; k=6) 
Global Max Pooling: 	73.19% (+/- 3.35%)  (PCA=20; k=4)
